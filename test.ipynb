{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Coursera - IBM Data Science Capstone Project ##", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Brian Lee ###\nBrianTrewLee@gmail.com\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "LinkedIn: https://www.linkedin.com/in/brian-lee-64a2022b/", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Introduction ##", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Where in the world can you find a StarBuck's? Well... everywhere... almost. Is there a science to locating a StarBuck's? Can we create a model to predict where a StarBuck's is likely to be located?", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Solving environment: | "
                }
            ], 
            "source": "# import libraries\n\nimport urllib.request # open and read URLs\n\nimport json # handle JSON files\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\nimport requests # handle requests\nimport pandas as pd # process data as dataframes with Pandas\nimport numpy as np # handle data in a vectorized manner with NumPy\n\n# !conda install -c conda-forge geopy --yes # uncomment this line if you haven't installed the GeoPy geocoding library yet\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't installed the Folium library yet\nimport folium # map rendering library\n\n# Matplolib plotting library and associated modules\nimport matplotlib.pyplot as plt \nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import KMeans # for K-Means clustering with Scikit-Learn\n\nprint(\"Libraries imported!\")"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n\ndef getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=750, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=2000, limit=100):\n    \n    count = 0\n    venues_list = []\n    for lat, lng in zip(latitudes, longitudes):\n        print(lat,'-',lng)\n         \n        #create a unique namefor this pointthat is easier todeal with than a combination of lat and lon\n        point_name = \"point\"+str(count)\n            \n        # create the API request URL\n        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            limit)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n        #print(\"resultscols=\",results.columns)\n        \n\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            point_name,\n            lat, \n            lng, \n            v[\"venue\"][\"name\"], \n            v[\"venue\"][\"location\"][\"lat\"], \n            v[\"venue\"][\"location\"][\"lng\"],  \n            v[\"venue\"][\"location\"][\"distance\"], \n            v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])\n        count = count + 1\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = [\"Point Name\",\n                  \"Neighborhood Latitude\", \n                  \"Neighborhood Longitude\", \n                  \"Venue\", \n                  \"Venue Latitude\", \n                  \"Venue Longitude\", \n                  \"Venue Distance\",           \n                  \"Venue Category\"]\n    \n    return(nearby_venues)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "sf_lat=37.7749\nsf_lon=-122.4194\nch_lat=41.8781\nch_lon=-87.6298\nla_lat=34.0522\nla_lon=-118.2437\nstarbucks_m_lat=37.744644\nstarbucks_m_lon=-122.452731\nimport random\n\n#37.780253, -122.509117\n#37.804203, -122.459264 - upper left of starbucks intensive space\n#37.767547, -122.409342 - lower right ofstarbucksintensive space\nllat=37.767547\nulat=37.804203\nllon=-122.459264\nulon=-122.409342\nrandom_lat0 = random.uniform(37.767547,37.804203)\nrandom_lon0 = random.uniform(-122.459264, -122.409342)\nrandom_lat1 = random.uniform(llat,ulat)\nrandom_lon1 = random.uniform(llon, ulon)\nrandom_lat2 = random.uniform(llat,ulat)\nrandom_lon2 = random.uniform(llon, ulon)\nrandom_lat3 = random.uniform(llat,ulat)\nrandom_lon3 = random.uniform(llon, ulon)\nrandom_lat4 = random.uniform(llat,ulat)\nrandom_lon4 = random.uniform(llon, ulon)\nrandom_lat5 = random.uniform(llat,ulat)\nrandom_lon5 = random.uniform(llon, ulon)\nrandom_lat6 = random.uniform(llat,ulat)\nrandom_lon6 = random.uniform(llon, ulon)\nrandom_lat7 = random.uniform(llat,ulat)\nrandom_lon7 = random.uniform(llon, ulon)\nrandom_lat8 = random.uniform(llat,ulat)\nrandom_lon8 = random.uniform(llon, ulon)\nrandom_lat9 = random.uniform(llat,ulat)\nrandom_lon9 = random.uniform(llon, ulon)\n\ndata = pd.DataFrame(np.array([[sf_lat, sf_lon], [ch_lat, ch_lon], [la_lat, la_lon], [starbucks_m_lat,starbucks_m_lon], [random_lat0,random_lon0], [random_lat1,random_lon1], [random_lat2,random_lon2], [random_lat3,random_lon3], [random_lat4,random_lon4], [random_lat5,random_lon5], [random_lat6,random_lon6], [random_lat7,random_lon7], [random_lat8,random_lon8], [random_lat9,random_lon9] ]), columns=['Latitude', 'Longitude'])\n\ndata"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues = getNearbyVenues(latitudes = data[\"Latitude\"],\n                               longitudes = data[\"Longitude\"]\n                               )\nvenues"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues['Venue'].unique()\nvenues['Venue'].value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues[350:400]\n\n\n# one hot encoding\nonehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\nonehot['point_name'] = venues['Point Name'] \nonehot['point_lat'] = venues['Neighborhood Latitude'] \nonehot['point_lon'] = venues['Neighborhood Longitude'] \nonehot['isStarbucks'] = (venues['Venue'] == \"Starbucks\")\n# move neighborhood column to the first column\nfixed_columns = [onehot.columns[-3]] + [onehot.columns[-2]] + [onehot.columns[-1]] + list(onehot.columns[:-3])\nonehot = onehot[fixed_columns]\n\nonehot.isStarbucks = onehot.isStarbucks.astype(int)\n\nonehot.head()\nonehot['isStarbucks'].value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "grouped = onehot.groupby('point_name').mean().reset_index()\ngrouped"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_top_venues = 50\n\nfor point in grouped['point_name']:\n    print(\"----\"+point+\"----\")\n    temp = grouped[grouped['point_name'] == point].T.reset_index()\n    temp.columns = ['venue','freq']\n    temp = temp.iloc[2:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n    print('\\n')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "grouped.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y = grouped['isStarbucks'] > 0\ny = pd.DataFrame(y)\ny = y.isStarbucks.astype(int)\nprint(y.value_counts())\nX = grouped\nprint(X.shape)\nX .drop(columns=['point_name', 'point_lat', 'point_lon', 'isStarbucks'], inplace=True)\nprint(X.columns)\nprint(X.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import train_test_split\n# create training and testing vars\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sklearn as sk  \nfrom sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)  \nRF.fit(X, y)  \nRF.predict(X)  \nRF.score(X,y)"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}