{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "## Coursera - IBM Data Science Capstone Project ##", 
            "cell_type": "markdown", 
            "metadata": {
                "collapsed": true
            }
        }, 
        {
            "source": "### Brian Lee ###\nBrianTrewLee@gmail.com<br />\nLinkedIn: https://www.linkedin.com/in/brian-lee-64a2022b/\n\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "A description of the problem and a discussion of the background. (15 marks)<br />\nA description of the data and how it will be used to solve the problem. (15 marks)", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "## 1. Introduction ##", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "If I were to open a Starbucks, where should I locate it? Clearly, the location is one of the most important business decisions for this venture. Is there a science to locating a StarBucks? Can we create a machine learning model to predict where a StarBucks is likely to be located? I'd like to find out.\n\nIf a reliable model can be made,it could be used in the process of opening a store. It could be used as a final sanity check, or at the beginning stage, to select a promising location. If a trustworthy model predicts with a high degree of confidence that a Starbucks should be located in an area, but there is not one there yet, perhaps that is an opportunity.\n\nAny person interested in opening a Starbucks should be interested in these results. This includes myself, other potential franchisees, and the Starbucks corporation itself, which operates many of it's own stores. The Starbucks competition might also be interested as they could possibly gain competitive insigths. I also believe others might be interested in this procedure, as it might be applied to predicting the location of other entities, based on the same sort of data.\n\nThe main purpose of this project is to prove the concept of predicting Starbuck's locations in general. I suspect it may work better or worse depending on the locations chosen for training and prediction, the radius size, and the specific features used in the model. I may vary those factors in order to prove the concept, which could then be applied carefully to a particular geographic area of interest at a later date.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 2. Data ###", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "Foursquare (https://foursquare.com) is a company that crowdsources location data, tying latitude-longitude coordinates (and other things) with public venues, including many businesses, such as Starbucks locations. The features for the Starbucks location classifier will come from venue data from the Fourquare API. I will use venue category names (such as 'bar', 'Chinese restaurant', 'coffee shop') as features to classify an area as either an area with a Starbucks, or an area without one. I will use the venue name (e.g. 'Starbucks') to determine whether or not the venue is a Starbucks. An area containing a Starbucks contains at least one venue named Starbucks within the radius supplied to the Foursquare API 'explore' endpoint.\n\nAn area will be a circular area with a given radius. I have not determined that radius yet, and I might use more than one radius in this project. Within that radius, the Foursquare API will return venues. The larger the radius, the more venues that could be anticipated, and the higher likelihood of a Starbucks, but a larger radius is also less useful for business use. I have done some exploratory work with a radius of 300m.\n\nThe areas to be chosen should contain a mix of areas with Starbuck's and those without. I have done some exploratory work in a particular area of San Francisco. I located Starbucks with Google Maps and got latitude and longitude points making a box around that area. I could choose the train and test points at random within an area, or I could generate a grid covering an entire area. Given the constraints my Foursquare Developer account places on my searches, and knowing that binary classifiers work better with a good number of examples of both classes, I may want to generate areas that I know will be heavy with Starbucks locations. To that end, I discovered a Kaggle dataset (https://www.kaggle.com/starbucks/store-locations) containing the latitude and longitude coordinates of 25,600 worldwide Starbucks locations. I may or may not use this resource.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "### 3. Methodology ###\n\nI will generate a corpus of geographic points, and using a reasonable radius (perhaps 300m) a corpus of geographic areas. Each geographic area will have a feature set based on json data returned by the Foursquare API's 'explore' endpoint. I plan to use the category names of the returned venues as features for that area. I will use the venue names to determine whether oir not a Starbucks is in that area, and generate the labels from that. \n\nOnce I have labelled data, I will use sklearn.model_selection.train_test_split to generate tarining and testing sets. Then I  can use any of a host of sklearn classifiers to generate predictive models. I have done some exploratory work with a random forest.", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Solving environment: done\n\n## Package Plan ##\n\n  environment location: /opt/conda/envs/DSX-Python35\n\n  added / updated specs: \n    - folium=0.5.0\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    vincent-0.4.4              |             py_1          28 KB  conda-forge\n    folium-0.5.0               |             py_0          45 KB  conda-forge\n    openssl-1.0.2r             |       h14c3975_0         3.1 MB  conda-forge\n    branca-0.3.1               |             py_0          25 KB  conda-forge\n    altair-2.2.2               |           py35_1         462 KB  conda-forge\n    ca-certificates-2019.3.9   |       hecc5488_0         146 KB  conda-forge\n    certifi-2018.8.24          |        py35_1001         139 KB  conda-forge\n    ------------------------------------------------------------\n                                           Total:         4.0 MB\n\nThe following NEW packages will be INSTALLED:\n\n    altair:          2.2.2-py35_1      conda-forge\n    branca:          0.3.1-py_0        conda-forge\n    folium:          0.5.0-py_0        conda-forge\n    vincent:         0.4.4-py_1        conda-forge\n\nThe following packages will be UPDATED:\n\n    ca-certificates: 2019.1.23-0                   --> 2019.3.9-hecc5488_0 conda-forge\n    certifi:         2018.8.24-py35_1              --> 2018.8.24-py35_1001 conda-forge\n\nThe following packages will be DOWNGRADED:\n\n    openssl:         1.0.2s-h7b6447c_0             --> 1.0.2r-h14c3975_0   conda-forge\n\n\nDownloading and Extracting Packages\nvincent-0.4.4        | 28 KB     | ##################################### | 100% \nfolium-0.5.0         | 45 KB     | ##################################### | 100% \nopenssl-1.0.2r       | 3.1 MB    | ##################################### | 100% \nbranca-0.3.1         | 25 KB     | ##################################### | 100% \naltair-2.2.2         | 462 KB    | ##################################### | 100% \nca-certificates-2019 | 146 KB    | ##################################### | 100% \ncertifi-2018.8.24    | 139 KB    | ##################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\nLibraries imported!\n"
                }
            ], 
            "source": "# import libraries\n\nimport urllib.request # open and read URLs\n\nimport json # handle JSON files\nfrom pandas.io.json import json_normalize # tranform JSON file into a pandas dataframe\n\nimport requests # handle requests\nimport pandas as pd # process data as dataframes with Pandas\nimport numpy as np # handle data in a vectorized manner with NumPy\n\n# !conda install -c conda-forge geopy --yes # uncomment this line if you haven't installed the GeoPy geocoding library yet\nfrom geopy.geocoders import Nominatim # convert an address into latitude and longitude values\n\n!conda install -c conda-forge folium=0.5.0 --yes # uncomment this line if you haven't installed the Folium library yet\nimport folium # map rendering library\n\n# Matplolib plotting library and associated modules\nimport matplotlib.pyplot as plt \nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n\nfrom sklearn.cluster import KMeans # for K-Means clustering with Scikit-Learn\n\nprint(\"Libraries imported!\")"
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# The code was removed by Watson Studio for sharing."
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Define a function to gather recommended venues, with specifically the name and category, using the explore API\n\ndef getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=300, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=750, limit=100):\n#def getNearbyVenues(latitudes, longitudes, radius=2000, limit=100):\n    \n    count = 0\n    venues_list = []\n    for lat, lng in zip(latitudes, longitudes):\n        #create a unique namefor this pointthat is easier todeal with than a combination of lat and lon\n        point_name = \"point\"+str(count)\n        print(point_name,':',lat,'-',lng)\n         \n\n            \n        # create the API request URL\n        url = \"https://api.foursquare.com/v2/venues/explore?&client_id={}&client_secret={}&v={}&ll={},{}&radius={}&limit={}\".format(\n            CLIENT_ID, \n            CLIENT_SECRET, \n            VERSION, \n            lat, \n            lng, \n            radius, \n            limit)\n            \n        # make the GET request\n        results = requests.get(url).json()[\"response\"][\"groups\"][0][\"items\"]\n        #print(\"resultscols=\",results.columns)\n        \n\n        \n        # return only relevant information for each nearby venue\n        venues_list.append([(\n            point_name,\n            lat, \n            lng, \n            v[\"venue\"][\"name\"], \n            v[\"venue\"][\"location\"][\"lat\"], \n            v[\"venue\"][\"location\"][\"lng\"],  \n            v[\"venue\"][\"location\"][\"distance\"], \n            v[\"venue\"][\"categories\"][0][\"name\"]) for v in results])\n        count = count + 1\n    nearby_venues = pd.DataFrame([item for venue_list in venues_list for item in venue_list])\n    nearby_venues.columns = [\"Point Name\",\n                  \"Neighborhood Latitude\", \n                  \"Neighborhood Longitude\", \n                  \"Venue\", \n                  \"Venue Latitude\", \n                  \"Venue Longitude\", \n                  \"Venue Distance\",           \n                  \"Venue Category\"]\n    \n    return(nearby_venues)"
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Latitude</th>\n      <th>Longitude</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>37.774900</td>\n      <td>-122.419400</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>41.878100</td>\n      <td>-87.629800</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>34.052200</td>\n      <td>-118.243700</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>37.744644</td>\n      <td>-122.452731</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>37.777190</td>\n      <td>-122.413290</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>37.784248</td>\n      <td>-122.443936</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>37.798106</td>\n      <td>-122.420917</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>37.769420</td>\n      <td>-122.421316</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>37.790511</td>\n      <td>-122.453908</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>37.787545</td>\n      <td>-122.456537</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>37.789597</td>\n      <td>-122.445566</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>37.777097</td>\n      <td>-122.436968</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>37.784641</td>\n      <td>-122.443591</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>37.768294</td>\n      <td>-122.410251</td>\n    </tr>\n  </tbody>\n</table>\n</div>", 
                        "text/plain": "     Latitude   Longitude\n0   37.774900 -122.419400\n1   41.878100  -87.629800\n2   34.052200 -118.243700\n3   37.744644 -122.452731\n4   37.777190 -122.413290\n5   37.784248 -122.443936\n6   37.798106 -122.420917\n7   37.769420 -122.421316\n8   37.790511 -122.453908\n9   37.787545 -122.456537\n10  37.789597 -122.445566\n11  37.777097 -122.436968\n12  37.784641 -122.443591\n13  37.768294 -122.410251"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "sf_lat=37.7749\nsf_lon=-122.4194\nch_lat=41.8781\nch_lon=-87.6298\nla_lat=34.0522\nla_lon=-118.2437\nstarbucks_m_lat=37.744644\nstarbucks_m_lon=-122.452731\nimport random\n\n#37.780253, -122.509117\n#37.804203, -122.459264 - upper left of starbucks intensive space\n#37.767547, -122.409342 - lower right ofstarbucksintensive space\nllat=37.767547\nulat=37.804203\nllon=-122.459264\nulon=-122.409342\nrandom_lat0 = random.uniform(37.767547,37.804203)\nrandom_lon0 = random.uniform(-122.459264, -122.409342)\nrandom_lat1 = random.uniform(llat,ulat)\nrandom_lon1 = random.uniform(llon, ulon)\nrandom_lat2 = random.uniform(llat,ulat)\nrandom_lon2 = random.uniform(llon, ulon)\nrandom_lat3 = random.uniform(llat,ulat)\nrandom_lon3 = random.uniform(llon, ulon)\nrandom_lat4 = random.uniform(llat,ulat)\nrandom_lon4 = random.uniform(llon, ulon)\nrandom_lat5 = random.uniform(llat,ulat)\nrandom_lon5 = random.uniform(llon, ulon)\nrandom_lat6 = random.uniform(llat,ulat)\nrandom_lon6 = random.uniform(llon, ulon)\nrandom_lat7 = random.uniform(llat,ulat)\nrandom_lon7 = random.uniform(llon, ulon)\nrandom_lat8 = random.uniform(llat,ulat)\nrandom_lon8 = random.uniform(llon, ulon)\nrandom_lat9 = random.uniform(llat,ulat)\nrandom_lon9 = random.uniform(llon, ulon)\n\ndata = pd.DataFrame(np.array([[sf_lat, sf_lon], [ch_lat, ch_lon], [la_lat, la_lon], [starbucks_m_lat,starbucks_m_lon], [random_lat0,random_lon0], [random_lat1,random_lon1], [random_lat2,random_lon2], [random_lat3,random_lon3], [random_lat4,random_lon4], [random_lat5,random_lon5], [random_lat6,random_lon6], [random_lat7,random_lon7], [random_lat8,random_lon8], [random_lat9,random_lon9] ]), columns=['Latitude', 'Longitude'])\n\ndata\n\n"
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "random_latlons = []\nfor j in range(0,300):\n    random_lat = random.uniform(llat,ulat)\n    random_lon = random.uniform(llon,ulon)\n    random_latlon=(random_lat,random_lon)\n    random_latlons.append(random_latlon)\n    "
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(300,)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "random_latlons = pd.DataFrame(random_latlons)\nrandom_lats=random_latlons[0]\nrandom_lons=random_latlons[1]\nrandom_lats.shape\n\n"
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(300,)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "random_latlons[0].shape"
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(300,)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "random_latlons[1].shape"
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(14,)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "\n\ndata[\"Latitude\"] = random_latlons[0]\ndata[\"Longitude\"] = random_latlons[1]\n\ndata[\"Latitude\"].head()\ndata[\"Latitude\"].shape\n"
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "execution_count": 10, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "(14, 2)"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "data[\"Longitude\"].head()\ndata.shape"
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "point0 : 37.795583403873174 - -122.41182435055846\n"
                }, 
                {
                    "ename": "KeyError", 
                    "evalue": "'groups'", 
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", 
                        "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)", 
                        "\u001b[0;32m<ipython-input-11-c8fd2e7be544>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#                               )\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m venues = getNearbyVenues(latitudes = random_lats,\n\u001b[0;32m----> 6\u001b[0;31m                                \u001b[0mlongitudes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_lons\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m                                )\n", 
                        "\u001b[0;32m<ipython-input-3-d2c6e4b734d0>\u001b[0m in \u001b[0;36mgetNearbyVenues\u001b[0;34m(latitudes, longitudes, radius, limit)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# make the GET request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"groups\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"items\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;31m#print(\"resultscols=\",results.columns)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n", 
                        "\u001b[0;31mKeyError\u001b[0m: 'groups'"
                    ], 
                    "output_type": "error"
                }
            ], 
            "source": "\n#venues = getNearbyVenues(latitudes = data[\"Latitude\"],\n#                               longitudes = data[\"Longitude\"]\n#                               )\nvenues = getNearbyVenues(latitudes = random_lats,\n                               longitudes = random_lons\n                               )\n"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues.tail()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues['Venue'].unique()\nvenues['Venue'].value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues[350:400]\n\n\n# one hot encoding\nonehot = pd.get_dummies(venues[['Venue Category']], prefix=\"\", prefix_sep=\"\")\n\n# add neighborhood column back to dataframe\nonehot['point_name'] = venues['Point Name'] \nonehot['point_lat'] = venues['Neighborhood Latitude'] \nonehot['point_lon'] = venues['Neighborhood Longitude'] \nonehot['isStarbucks'] = (venues['Venue'] == \"Starbucks\")\n# move neighborhood column to the first column\nfixed_columns = [onehot.columns[-3]] + [onehot.columns[-2]] + [onehot.columns[-1]] + list(onehot.columns[:-3])\nonehot = onehot[fixed_columns]\n\nonehot.isStarbucks = onehot.isStarbucks.astype(int)\n\nonehot.head()\nonehot['isStarbucks'].value_counts()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "venues.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "grouped = onehot.groupby('point_name').mean().reset_index()\ngrouped"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "num_top_venues = 50\n\nfor point in grouped['point_name']:\n    print(\"----\"+point+\"----\")\n    temp = grouped[grouped['point_name'] == point].T.reset_index()\n    temp.columns = ['venue','freq']\n    temp = temp.iloc[2:]\n    temp['freq'] = temp['freq'].astype(float)\n    temp = temp.round({'freq': 2})\n    print(temp.sort_values('freq', ascending=False).reset_index(drop=True).head(num_top_venues))\n    print('\\n')"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "grouped.head()"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "y = grouped['isStarbucks'] > 0\ny = pd.DataFrame(y)\ny = y.isStarbucks.astype(int)\nprint(y.value_counts())\nX = grouped\nprint(X.shape)\nX .drop(columns=['point_name', 'point_lat', 'point_lon', 'isStarbucks'], inplace=True)\nprint(X.columns)\nprint(X.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.model_selection import train_test_split\n# create training and testing vars\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\nprint(X_train.shape, y_train.shape)\nprint(X_test.shape, y_test.shape)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sklearn as sk  \nfrom sklearn.ensemble import RandomForestClassifier\n\n#RF = RandomForestClassifier(n_estimators=100, max_depth=3, random_state=0)  \nRF = RandomForestClassifier(n_estimators=3, max_depth=30, random_state=0)  \nRF.fit(X, y)  \nRF.predict(X)  \nRF.score(X,y)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "estimator = RF.estimators_[2]"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "estimator"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "from sklearn.tree import export_graphviz\n# Export as dot file\nexport_graphviz(estimator, out_file='tree.dot', \n                feature_names = X.columns,\n                class_names = 'has_starbucks',\n                rounded = True, proportion = False, \n                precision = 2, filled = True)"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "# Convert to png using system command (requires Graphviz)\nfrom subprocess import call\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\n\n# Display in jupyter notebook\nfrom IPython.display import Image\nImage(filename = 'tree.png')"
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.5", 
            "name": "python3", 
            "language": "python"
        }, 
        "language_info": {
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "version": "3.5.5", 
            "name": "python", 
            "file_extension": ".py", 
            "pygments_lexer": "ipython3", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }
        }
    }, 
    "nbformat": 4
}